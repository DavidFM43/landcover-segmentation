{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsuWiRjyIHoC",
        "outputId": "8dcd0c95-5635-4e57-b575-c57fe5cc2c28"
      },
      "outputs": [],
      "source": [
        "! pip install segmentation_models_pytorch -q\n",
        "! git clone -b inference https://github.com/DavidFM43/landcover-segmentation.git\n",
        "%cd landcover-segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrGAl6oLIfk5"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"downsize_res\": 512,\n",
        "    \"batch_size\": 6,\n",
        "    \"epochs\": 30,\n",
        "    \"lr\": 3e-4,\n",
        "    \"model_architecture\": \"Unet\",\n",
        "    \"model_config\": {\n",
        "        \"encoder_name\": \"resnet34\",\n",
        "        \"encoder_weights\": \"imagenet\",\n",
        "        \"in_channels\": 3,\n",
        "        \"classes\": 7,\n",
        "\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvkxa11vLJKt",
        "outputId": "bb887422-0921-4c5f-9a6a-0e53b0b1bf46"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torchvision import transforms\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# instantiate model and load weights\n",
        "cp_path = \"checkpoints/CP_epoch20.pth\"\n",
        "model_architecture = getattr(smp, config[\"model_architecture\"])\n",
        "model = model_architecture(**config[\"model_config\"])\n",
        "model.load_state_dict(torch.load(cp_path, map_location=torch.device(device)))\n",
        "model.to(device)\n",
        "model.eval();\n",
        "\n",
        "downsize_res = config[\"downsize_res\"]\n",
        "# mean = [0.4085, 0.3798, 0.2822]\n",
        "# std = [0.1410, 0.1051, 0.0927]\n",
        "# transforms\n",
        "downsize_t = transforms.Resize(downsize_res, antialias=True)\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean, std),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "vWQQECPMMsid",
        "outputId": "90aba48d-70e9-4224-d251-45d7d0fca2e0"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "from PIL import Image\n",
        "\n",
        "images_dir = \"data/sample_sat_images\"\n",
        "image_ids = os.listdir(images_dir)\n",
        "sample_id = image_ids[0]\n",
        "image_path = f\"{images_dir}/{sample_id}\"\n",
        "sat_img = Image.open(image_path)\n",
        "sat_img.resize((512, 512))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz85q4d3Mu-g"
      },
      "outputs": [],
      "source": [
        "X = transform(sat_img).unsqueeze(0)\n",
        "X = X.to(device)\n",
        "X_down = downsize_t(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bMFlP1gN-R1",
        "outputId": "5c4c0350-a17e-428a-dc77-bb47dc818c7c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# forward pass\n",
        "logits = model(X_down)\n",
        "preds = torch.argmax(logits, 1).detach()\n",
        "# resize to evaluate with the original image\n",
        "preds = transforms.functional.resize(preds, X.shape[-2:], antialias=True)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0EfteEdONTr"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import draw_segmentation_masks\n",
        "from torchvision.io import read_image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def label_to_onehot(mask, num_classes):\n",
        "    dims_p = (2, 0, 1) if mask.ndim == 2 else (0, 3, 1, 2)\n",
        "    return torch.permute(\n",
        "        F.one_hot(mask.type(torch.long), num_classes=num_classes).type(torch.bool),\n",
        "        dims_p,\n",
        "    )\n",
        "\n",
        "sat_img = read_image(image_path)\n",
        "masks = preds.squeeze()\n",
        "masks = label_to_onehot(masks, 7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl5wtGQxPNkB"
      },
      "outputs": [],
      "source": [
        "class_rgb_colors = [(0, 255, 255),\n",
        " (255, 255, 0),\n",
        " (255, 0, 255),\n",
        " (0, 255, 0),\n",
        " (0, 0, 255),\n",
        " (255, 255, 255),\n",
        " (0, 0, 0)]\n",
        "\n",
        "mask_over_image = draw_segmentation_masks(\n",
        "    sat_img, masks=masks, alpha=0.2, colors=class_rgb_colors\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opnCFblQP6AS",
        "outputId": "df5ff028-e17e-4751-f99c-8683da36fb0e"
      },
      "outputs": [],
      "source": [
        "mask_over_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B6JvH2_3P5in",
        "outputId": "7637d9b2-868c-49e9-828d-531de4197391"
      },
      "outputs": [],
      "source": [
        "transforms.functional.to_pil_image(mask_over_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMPH1gTSUzMa"
      },
      "outputs": [],
      "source": [
        "def get_overlay(sat_img, preds, alpha):\n",
        "    class_rgb_colors = [(0, 255, 255), (255, 255, 0), (255, 0, 255), (0, 255, 0), (0, 0, 255), (255, 255, 255), (0, 0, 0)]\n",
        "    masks = preds.squeeze()\n",
        "    masks = label_to_onehot(masks, 7)\n",
        "    overlay = draw_segmentation_masks(\n",
        "        sat_img, masks=masks, alpha=alpha, colors=class_rgb_colors\n",
        "    )\n",
        "    return overlay\n",
        "\n",
        "def segment(sat_image):\n",
        "    sat_img = Image.open(image_path)\n",
        "    sat_img2 = read_image(image_path)\n",
        "    # preprocess image\n",
        "    X = transform(sat_img).unsqueeze(0)\n",
        "    X = X.to(device)\n",
        "    X_down = downsize_t(X)\n",
        "    # forward pass\n",
        "    logits = model(X_down)\n",
        "    preds = torch.argmax(logits, 1).detach()\n",
        "    # resize to evaluate with the original image\n",
        "    preds = transforms.functional.resize(preds, X.shape[-2:], antialias=True)        \n",
        "    overlay = get_overlay(sat_img2, preds, 0.2)\n",
        "    raw_masks = get_overlay(torch.zeros_like(sat_img2), preds, 1)\n",
        "\n",
        "    raw_masks = torch.permute(raw_masks, (1, 2, 0))\n",
        "    overlay = torch.permute(overlay, (1, 2, 0))\n",
        "\n",
        "    return raw_masks.numpy(), overlay.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "c29S_v9NQH7L",
        "outputId": "153bc359-ad61-4219-e6ca-2179a5860519"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "i = gr.inputs.Image()\n",
        "o = [gr.Image(), gr.Image()]\n",
        "\n",
        "examples = [f\"{images_dir}/{image_id}\" for image_id in image_ids]\n",
        "title = \"Satellite Images Landcover Segmentation\"\n",
        "description = \"Upload an image or select from examples to segment\"\n",
        "\n",
        "gr.Interface(segment, i, o, examples=examples, title=title, description=description).launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9RKBA8kTtH1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
