{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import resize, to_pil_image\n",
    "from torchvision.utils import draw_segmentation_masks, make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "images_dir = data_dir / \"images\"\n",
    "masks_dir = data_dir / \"masks\"\n",
    "num_classes = 7\n",
    "\n",
    "int2str = {\n",
    "    0: \"urban_land\",\n",
    "    1: \"agriculture_land\",\n",
    "    2: \"rangeland\",\n",
    "    3: \"forest_land\",\n",
    "    4: \"water\",\n",
    "    5: \"barren_land\",\n",
    "    6: \"unknown\",\n",
    "}\n",
    "int2rgb = {\n",
    "    0: (0, 255, 255),\n",
    "    1: (255, 255, 0),\n",
    "    2: (255, 0, 255),\n",
    "    3: (0, 255, 0),\n",
    "    4: (0, 0, 255),\n",
    "    5: (255, 255, 255),\n",
    "    6: (0, 0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def label_to_onehot(mask, num_classes):\n",
    "    \"\"\"\n",
    "    Transforms a label encoded tensor to one hot encoding.\n",
    "        Parameters:\n",
    "            mask: Torch tensor of shape (H, W)\n",
    "            num_classes: Total number of classes:\n",
    "        Returns:\n",
    "            Torch tensor of shape (num_classes, H, W).\n",
    "    \"\"\"\n",
    "    dims_p = (2, 0, 1) if mask.ndim == 2 else (0, 3, 1, 2)\n",
    "    return torch.permute(F.one_hot(mask.long(), num_classes=num_classes).bool(), dims_p)\n",
    "\n",
    "\n",
    "## Legend\n",
    "# Define the legend size, background color, and text parameters\n",
    "legend_width = 140\n",
    "legend_height = 190\n",
    "text_color = (0, 0, 0)  # black\n",
    "# Create a new image for the legend\n",
    "legend_image = Image.new(\"RGB\", (legend_width, legend_height), (220, 220, 220))\n",
    "draw = ImageDraw.Draw(legend_image)\n",
    "# Set the initial position for drawing rectangles and text\n",
    "x = 10\n",
    "y = 10\n",
    "# Draw rectangles and labels for each legend item\n",
    "for idx in range(num_classes):\n",
    "    name = int2str[idx]\n",
    "    color = int2rgb[idx]\n",
    "    draw.rectangle([(x, y), (x + 20, y + 20)], fill=color)\n",
    "    draw.text((x + 30, y), name, fill=text_color)\n",
    "    y += 30\n",
    "# Define the position to paste the legend onto the original image\n",
    "legend_position = (10, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from dataset import LandcoverDataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "images_dir = data_dir / \"images\"\n",
    "masks_dir = data_dir / \"masks\"\n",
    "\n",
    "\n",
    "ds = LandcoverDataset(transform=transforms.ToTensor(), target_transform=transforms.PILToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids, test_ids = torch.utils.data.random_split(\n",
    "    ds.image_ids, [454, 207, 142], generator=torch.Generator().manual_seed(42)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annot(ids):\n",
    "    class_per_mask = []\n",
    "    for id in tqdm(ids):\n",
    "        mask = read_image(str(masks_dir / f\"{id}_mask.png\"))\n",
    "        classes_in_image = torch.bincount(mask.view(-1), minlength=7) > 0\n",
    "        class_per_mask.append(classes_in_image.tolist())\n",
    "\n",
    "    annot = pd.DataFrame(class_per_mask, columns=int2str.values(), index=list(ids)).reset_index(names=[\"id\"])\n",
    "    return annot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:17<00:00, 26.09it/s]\n",
      "100%|██████████| 207/207 [00:07<00:00, 26.24it/s]\n",
      "100%|██████████| 142/142 [00:05<00:00, 25.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_annot = get_annot(train_ids)\n",
    "val_annot = get_annot(val_ids)\n",
    "test_annot = get_annot(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train\n",
      "Number of images with each class:\n",
      "\n",
      "urban_land          0.209\n",
      "agriculture_land    0.234\n",
      "rangeland           0.169\n",
      "forest_land         0.061\n",
      "water               0.151\n",
      "barren_land         0.129\n",
      "unknown             0.047\n",
      "dtype: float64 \n",
      "\n",
      "Split: val\n",
      "Number of images with each class:\n",
      "\n",
      "urban_land          0.192\n",
      "agriculture_land    0.218\n",
      "rangeland           0.172\n",
      "forest_land         0.073\n",
      "water               0.153\n",
      "barren_land         0.137\n",
      "unknown             0.056\n",
      "dtype: float64 \n",
      "\n",
      "Split: test\n",
      "Number of images with each class:\n",
      "\n",
      "urban_land          0.215\n",
      "agriculture_land    0.230\n",
      "rangeland           0.146\n",
      "forest_land         0.047\n",
      "water               0.151\n",
      "barren_land         0.157\n",
      "unknown             0.053\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for annot, split_name in zip([train_annot, val_annot, test_annot], [\"train\", \"val\", \"test\"]):\n",
    "    print(\"Split:\", split_name)\n",
    "    print(\"Number of images with each class:\\n\")\n",
    "    counts = annot.drop(columns=[\"id\"]).sum(0)\n",
    "    props = counts / counts.sum()\n",
    "    print(round(props, 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_label(n_images, class_name, annot, alpha=0.2, downsize_res=None):\n",
    "    \"\"\"\n",
    "    Visualize a sample of masks of the desired class.\n",
    "        Parameters:\n",
    "            n_images: Number of images in the sample\n",
    "            class_name: Class name string, one of [forest, rangeland, barren_land\n",
    "            water, agriculture_land, urban_land]\n",
    "            downsize_red: Resolution to downsize the images to\n",
    "    \"\"\"\n",
    "    class_images = annot[class_name]\n",
    "    ids = annot[\"id\"][class_images]\n",
    "    sample = np.random.choice(ids, n_images)\n",
    "\n",
    "    imgs = []\n",
    "    for img_id in sample:\n",
    "        sat_img = read_image(str(images_dir / f\"{img_id}_sat.jpg\"))\n",
    "        raw_masks = read_image(str(masks_dir / f\"{img_id}_mask.png\")).squeeze()\n",
    "        if downsize_res is not None:\n",
    "            sat_img = resize(sat_img, downsize_res)\n",
    "            raw_masks = resize(raw_masks, downsize_res)\n",
    "        masks = label_to_onehot(raw_masks, 7)\n",
    "        mask_over_image = draw_segmentation_masks(sat_img, masks=masks, alpha=alpha, colors=list(int2rgb.values()))\n",
    "        imgs.extend([sat_img, mask_over_image])\n",
    "\n",
    "    grid = make_grid(imgs, nrow=2)\n",
    "\n",
    "    pil_image = to_pil_image(grid)\n",
    "    pil_image.paste(legend_image, legend_position)\n",
    "    return pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(viewnior:227424): Gtk-WARNING **: 17:21:11.065: Unable to locate theme engine in module_path: \"murrine\",\n",
      "\n",
      "(viewnior:227424): Gtk-WARNING **: 17:21:11.065: Unable to locate theme engine in module_path: \"murrine\",\n",
      "\n",
      "(viewnior:227424): Gtk-WARNING **: 17:21:11.069: Unable to locate theme engine in module_path: \"murrine\",\n",
      "\n",
      "(viewnior:227424): Gtk-WARNING **: 17:21:11.069: Unable to locate theme engine in module_path: \"murrine\",\n",
      "\n",
      "(viewnior:227424): Gtk-WARNING **: 17:21:11.069: Unable to locate theme engine in module_path: \"murrine\",\n"
     ]
    }
   ],
   "source": [
    "viz_label(4, \"rangeland\", annot=train_annot, alpha=0.2).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f61c2089b08795400945c903da408432651bbaca6b22bb6c13ea905da80bc86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
